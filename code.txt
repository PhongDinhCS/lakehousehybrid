https://repo1.maven.org/maven2/io/trino/trino-jdbc/478/
ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker cp /home/ubuntu2020/Downloads/trino-jdbc-478.jar nifi-nifi-1:/opt/nifi/nifi-current/lib/
Successfully copied 12.9MB to nifi-nifi-1:/opt/nifi/nifi-current/lib/

ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker exec -it nifi-nifi-1 ls /opt/nifi/nifi-current/lib | grep trino
ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker cp trino-jdbc-478.jar nifi-nifi-1:/opt/nifi/nifi-current/lib/
Successfully copied 12.9MB to nifi-nifi-1:/opt/nifi/nifi-current/lib/

ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker exec -it nifi-nifi-1 ls /opt/nifi/current/lib | grep trino
ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker exec -it nifi-nifi-1 ls /opt/nifi/nifi-current/lib | grep trino
trino-jdbc-478.jar
ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker restart nifi-nifi-1
nifi-nifi-1
ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ 

ubuntu2020@ubuntu2020-virtual-machine:~$ docker exec -it trino-coordinator bash
trino@trino-coordinator:/$ ls
bin  boot  data  dev  etc  home  lib  lib32  lib64  libx32  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
trino@trino-coordinator:/$ rm -rf /tmp/delta/hyb_bronze
trino@trino-coordinator:/$ ls -R /tmp/delta
ls: cannot access '/tmp/delta': No such file or directory
trino@trino-coordinator:/$ 


ubuntu2020@ubuntu2020-virtual-machine:~/lakehousehybrid/nifi$ jupyter notebook --port=8888 --ip=0.0.0.0 --no-browser

docker logs delta-spark


Using Python version 3.10.12 (main, May 27 2025 17:12:29) Spark context Web UI available at http://delta-spark:4041 Spark context available as 'sc' (master = local[*], app id = local-1765037795220). SparkSession available as 'spark'. >>> from pyspark.sql import SparkSession >>> >>> spark = SparkSession.builder.getOrCreate() >>> >>> hadoop_conf = spark._jsc.hadoopConfiguration() >>> hadoop_conf.set("fs.s3a.access.key", "admin") >>> hadoop_conf.set("fs.s3a.secret.key", "password") >>> hadoop_conf.set("fs.s3a.endpoint", "http://minio:9000") >>> hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") >>> hadoop_conf.set("fs.s3a.path.style.access", "true") >>> hadoop_conf.set("fs.s3a.connection.ssl.enabled", "false") >>> >>> df = spark.read.parquet("s3a://warehouse/fs/ecg_raw/day=20251206/") 25/12/06 23:17:22 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder". SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. >>>


bundle-2.40.0.jar
hadoop-aws-3.4.1.jar


from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

hadoop_conf = spark._jsc.hadoopConfiguration()
hadoop_conf.set("fs.s3a.access.key", "admin")
hadoop_conf.set("fs.s3a.secret.key", "password")
hadoop_conf.set("fs.s3a.endpoint", "http://minio:9000")
hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
hadoop_conf.set("fs.s3a.path.style.access", "true")
hadoop_conf.set("fs.s3a.connection.ssl.enabled", "false")

Spark: 4.0.0

Hadoop: 3.4.1

Delta Lake: 4.0.0 (delta-spark)

JARs: hadoop-aws-3.4.1.jar and bundle-2.40.0.jar present

