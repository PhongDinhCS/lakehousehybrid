ubuntu2020@ubuntu2020-virtual-machine:~/lakehousehybrid/airflow$ sudo chown -R 50000:50000 logs dags
[sudo] password for ubuntu2020: 
ubuntu2020@ubuntu2020-virtual-machine:~/lakehousehybrid/airflow$ sudo chown -R 50000:50000 logs dags
ubuntu2020@ubuntu2020-virtual-machine:~/lakehousehybrid/airflow$ ls -ld logs dags
drwxrwxr-x 3 50000 50000 4096 Thg 12 21 20:26 dags
drwxrwxr-x 7 50000 50000 4096 Thg 12 21 21:16 logs
ubuntu2020@ubuntu2020-virtual-machine:~/lakehousehybrid/airflow$ 



https://repo1.maven.org/maven2/io/trino/trino-jdbc/478/
ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker cp /home/ubuntu2020/Downloads/trino-jdbc-478.jar nifi-nifi-1:/opt/nifi/nifi-current/lib/
Successfully copied 12.9MB to nifi-nifi-1:/opt/nifi/nifi-current/lib/

ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker exec -it nifi-nifi-1 ls /opt/nifi/nifi-current/lib | grep trino
ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker cp trino-jdbc-478.jar nifi-nifi-1:/opt/nifi/nifi-current/lib/
Successfully copied 12.9MB to nifi-nifi-1:/opt/nifi/nifi-current/lib/

ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker exec -it nifi-nifi-1 ls /opt/nifi/current/lib | grep trino
ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker exec -it nifi-nifi-1 ls /opt/nifi/nifi-current/lib | grep trino
trino-jdbc-478.jar
ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ docker restart nifi-nifi-1
nifi-nifi-1
ubuntu2020@ubuntu2020-virtual-machine:~/Downloads$ 

ubuntu2020@ubuntu2020-virtual-machine:~$ docker exec -it trino-coordinator bash
trino@trino-coordinator:/$ ls
bin  boot  data  dev  etc  home  lib  lib32  lib64  libx32  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
trino@trino-coordinator:/$ rm -rf /tmp/delta/hyb_bronze
trino@trino-coordinator:/$ ls -R /tmp/delta
ls: cannot access '/tmp/delta': No such file or directory
trino@trino-coordinator:/$ 


ubuntu2020@ubuntu2020-virtual-machine:~/lakehousehybrid/nifi$ jupyter notebook --port=8888 --ip=0.0.0.0 --no-browser

docker logs delta-spark


Using Python version 3.10.12 (main, May 27 2025 17:12:29) Spark context Web UI available at http://delta-spark:4041 Spark context available as 'sc' (master = local[*], app id = local-1765037795220). SparkSession available as 'spark'. >>> from pyspark.sql import SparkSession >>> >>> spark = SparkSession.builder.getOrCreate() >>> >>> hadoop_conf = spark._jsc.hadoopConfiguration() >>> hadoop_conf.set("fs.s3a.access.key", "admin") >>> hadoop_conf.set("fs.s3a.secret.key", "password") >>> hadoop_conf.set("fs.s3a.endpoint", "http://minio:9000") >>> hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") >>> hadoop_conf.set("fs.s3a.path.style.access", "true") >>> hadoop_conf.set("fs.s3a.connection.ssl.enabled", "false") >>> >>> df = spark.read.parquet("s3a://warehouse/fs/ecg_raw/day=20251206/") 25/12/06 23:17:22 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder". SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. >>>


bundle-2.40.0.jar
hadoop-aws-3.4.1.jar


from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

hadoop_conf = spark._jsc.hadoopConfiguration()
hadoop_conf.set("fs.s3a.access.key", "admin")
hadoop_conf.set("fs.s3a.secret.key", "password")
hadoop_conf.set("fs.s3a.endpoint", "http://minio:9000")
hadoop_conf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
hadoop_conf.set("fs.s3a.path.style.access", "true")
hadoop_conf.set("fs.s3a.connection.ssl.enabled", "false")

spark.read.parquet("s3a://warehouse/fs/ecg_raw/day=20251206/").show()

Spark: 4.0.0

Hadoop: 3.4.1

Delta Lake: delta-spark_2.13-4.0.0.jar

JARs: hadoop-aws-3.4.1.jar and bundle-2.40.0.jar present

wget https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.0/delta-spark_2.13-4.0.0.jar -P /home/ubuntu2020/.local/lib/python3.10/site-packages/pyspark/jars/


from pyspark.sql import SparkSession

# =========================
# Create a new SparkSession (Delta + Hive + MinIO)
# =========================
spark = SparkSession.builder \
    .appName("Delta + S3 + Hive Full Config") \
    .config("spark.jars.packages", "io.delta:delta-spark_2.13:4.0.0,org.apache.hadoop:hadoop-aws:3.4.1") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://127.0.0.1:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "admin") \
    .config("spark.hadoop.fs.s3a.secret.key", "password") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
    .config("spark.sql.warehouse.dir", "s3a://warehouse/") \
    .config("spark.sql.catalogImplementation", "hive") \
    .config("spark.hadoop.hive.metastore.uris", "thrift://127.0.0.1:9083") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .enableHiveSupport() \
    .getOrCreate()

# =========================
# Verify the session
# =========================
print("Spark version:", spark.version)
print("Catalog implementation:", spark.conf.get("spark.sql.catalogImplementation"))
print("Active session:", SparkSession.getActiveSession())

spark.sql("SHOW DATABASES").show()

docker exec -it delta-spark bash

pyspark \
  --master local[*] \
  --packages io.delta:delta-spark_2.13:4.0.0,org.apache.hadoop:hadoop-aws:3.4.1 \
  --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
  --conf spark.hadoop.fs.s3a.access.key=admin \
  --conf spark.hadoop.fs.s3a.secret.key=password \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
  --conf spark.sql.warehouse.dir=s3a://warehouse/ \
  --conf spark.sql.catalogImplementation=hive \
  --conf spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083 \
  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog


spark.sql("SHOW TABLES IN hyb_bronze").show()
spark.sql("""
CREATE TABLE hyb_bronze.test_table (
    id INT,
    value STRING,
    created_at BIGINT
)
USING DELTA
LOCATION 's3a://warehouse/hyb_bronze.db/test_table'
""")

spark.sql("""
INSERT INTO hyb_bronze.test_table
VALUES
  (1, 'alpha', 1733725000000),
  (2, 'beta', 1733726000000)
""")

spark.sql("SELECT * FROM hyb_bronze.test_table").show()

docker network prune -f
docker compose down --remove-orphans
docker compose up -d




